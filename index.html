<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Ditail</title>
<link href="./style.css" rel="stylesheet">
<link rel="icon" href="./assets/logo.png" type="image/x-icon">
</head>

<body>
<div class="content">
  <h1><strong>Diffusion Cocktailüç∏: Fused Generation from Diffusion Models</strong></h1>
  <p id="authors"><a href="https://hmdliu.site/" target="_blank">Haoming Liu</a> <a href="https://ricercarg.github.io/" target="_blank">Yuanhe Guo</a> <a href="https://sheng-jie-wang.github.io/" target="_blank">Shengjie Wang</a> <a href="https://whongyi.github.io/" target="_blank">Hongyi Wen</a>
    <br><br>
    <span style="font-size: 20px"><a href="https://dail.shanghai.nyu.edu/" target="_blank">Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning, NYU Shanghai</a></span>
  </p>
  <br>
  <img src="./assets/header.png" class="teaser-gif" style="width:100%;">
    <font size="+2">
      <p style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.08873" target="_blank">Paper</a> &nbsp;&nbsp;&nbsp;&nbsp;
	      <a href="https://github.com/MAPS-research/Ditail" target="_blank">Code</a> &nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://huggingface.co/spaces/MAPS-research/Diffusion-Cocktail" target="_blank">Demo</a> &nbsp;&nbsp;&nbsp;&nbsp;
        <a href="./assets/bibtex.txt" target="_blank">BibTeX</a>
        </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Diffusion models excel at generating high-quality images and are easy to extend, making them extremely popular among active users who have created an extensive collection of diffusion models with various styles by fine-tuning base models such as Stable Diffusion. Recent work has focused on uncovering semantic and visual information encoded in various components of a diffusion model, enabling better generation quality and more fine-grained control. However, those methods target improving a single model and overlook the vastly available collection of fine-tuned diffusion models. In this work, we study the combinations of diffusion models. We propose <b>Di</b>ffusion Cock<b>tail</b> (<b>Ditail</b>), a training-free method that can accurately transfer content information between two diffusion models. This allows us to perform diverse generations using a set of diffusion models, resulting in novel images that are unlikely to be obtained by a single model alone. We also explore utilizing Ditail for style transfer, with the target style set by a diffusion model instead of an image. Ditail offers a more detailed manipulation of the diffusion generation, thereby enabling the vast community to integrate various styles and contents seamlessly and generate any content of any style.</p>
  <p><b>TL;DR:</b> Ditail offers a training-free method for novel image generations and fine-grained manipulations of content/style, enabling flexible integrations of existing pre-trained Diffusion models and LoRAs.</p>
</div>
<div class="content">
  <h2>Background</h2>
  <p>Diffusion models (DMs) have shown great success in generating high-quality images, conditioning on various input types (e.g., text, reference image, etc.). Even though many recent works have focused on improving a single DM on generation controllability and interpretability of model components, rarely have people studied combinations of diffusion models, ignoring the vast collection of DMs created by everyday users. We propose Diffusion Cocktail (Ditail) that is readily applicable to existing DMs without further training/fine-tuning, thus allowing us to utilize the abundant and fast-growing DM resources efficiently and effectively. Ditail enables us to accurately transfer the content/structure information between two DMs during the diffusion process. Moreover, Ditail is extensible to the style transfer of an input image that is not necessarily generated by a DM.</p>
</div>
<div class="content">
  <h2>Method Overview</h2>
  <p>Ditail adopts a two-stage approach to produce the target image. We first invert the source image into noisy latents, conditioning on a scaled interpolation of the prompt text embeddings (i.e., positive and negative prompts). Then, we perform denoising with the target DM while injecting the feature and self-attention maps (reconstructed from the source latents) into certain U-Net layers.</p>
  <img class="summary-img" src="./assets/overview.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Ditail for Novel Generation and Style Transfer</h2>
  <p>The Ditail content injection allows us to generate new images by mixing the representations of two DMs. Given a set of <b>m</b> DMs, we can use Ditail to generate <b>m<sup>2</sup></b> images as opposed to the <b>m</b>  images by running every DM independently. Note that this dramatically expands the generation capacity, producing images that are not attainable through a single DM. It also allows the user to have better control over the generation process as well as the integration of more domain-specific DMs. The user may first use some DMs to generate specific content structures and then transfer the spatial information to other DMs with desirable styles.</p>
  <img class="summary-img" src="./assets/style_matrix.png" style="width:70%;"> <br>
</div>
<div class="content">
  <h2>Ditail for Stylized Real-world Image Editing</h2>
  <p>Ditail enables stylized real-world image editing as well. For each image pair within a prompt group, we use a smaller guidance scale for the left image and a larger guidance scale for the other. The appearance and semantic class of the objects can be modified effectively, even with completely unrelated prompts (e.g., man to chicken). Note that changing the number of objects may still fail due to the emphasis on structure.</p>
  <img class="summary-img" src="./assets/real_edit.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Ablation Study on Condition Scaling and Content Injection</h2>
  <!-- <p>Ditail preserves the content/sturcture of the source image via condition scaling and content injection. Notably, the presented real-world images are drawn from the <a href="https://huggingface.co/spaces/MAPS-research/Diffusion-Cocktail-Demo" target="_blank">COCO Caption 2017</a> dataset and Œ± corresponds to the scaling factor for the positive prompt embedding. The list of DMs can be found in acknowledgements.</p> -->
  <img class="summary-img" src="./assets/ablation.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>More Qualitative Results (DM+LoRA) on Real-world Images</h2>
  <img class="summary-img" src="./assets/lora.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Controllability, Extensibility, and Efficency</h2>
  <p>Ditail offers diverse controls over the fused generation process, including content manipulation (positive and negative prompts), structure preservation (scaling factors and content injection), style (DMs and LoRAs), and granularity (edit steps and cfg factor). Moreover, Ditail is applicable to thousands of DMs and LoRAs available online, thereby enabling the vast community to <b>generate any content of any style</b>. With an Nvidia A10G-small GPU (i.e., our <a href="https://huggingface.co/spaces/MAPS-research/Diffusion-Cocktail" target="_blank">demo</a> on HuggingFace), processing a single image takes ~20 seconds for 50 edit steps using 16-bit floating point precision. Please refer to our paper and code for more technical details.</p>
</div>
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements:</strong>
    This work is supported in part by the Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning at NYU Shanghai, NYU Shanghai Boost Fund, and NYU HPC resources and services. We use the following DM and LoRA checkpoints: <a href="https://civitai.com/models/32333?modelVersionId=38765" target="_blank">BluePastel</a>, <a href="hhttps://civitai.com/models/91534?modelVersionId=97557" target="_blank">Chaos3.0</a>, <a href="https://civitai.com/models/75949?modelVersionId=87747" target="_blank">DiaMix</a>, <a href="https://civitai.com/models/15250?modelVersionId=96373" target="_blank">Little Illustration</a>, <a href="https://civitai.com/models/4201?modelVersionId=130072" target="_blank">Realistic Vision</a>, <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5" target="_blank">Stable Diffusion 1.5</a>, <a href="https://civitai.com/models/161450/pop-art" target="_blank">Pop</a>, <a href="https://www.liblib.art/modelinfo/76dcb8b59d814960b0244849f2747a15" target="_blank">Flat</a>, <a href="https://www.liblib.art/modelinfo/f732b23b02f041bdb7f8f3f8a256ca8b" target="_blank">Snow</a>. The presented source images are drawn from <a href="https://arxiv.org/abs/1504.00325v2" target="_blank">COCO Captions</a>, <a href="https://arxiv.org/abs/2210.08402" target="_blank">LAION-5B</a>, and <a href="https://arxiv.org/abs/2308.02205" target="_blank">GEMRec-18K</a>. The project page is adapted from <a href="https://dreambooth.github.io/" target="_blank">DreamBooth</a>.
  </p>
</div>
</body>
</html>
