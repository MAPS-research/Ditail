<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Ditail</title>
<link href="./style.css" rel="stylesheet">
<link rel="icon" href="./assets/logo.png" type="image/x-icon">
</head>

<body>
<div class="content">
  <h1><strong>Diffusion Cocktailüç∏: Mixing Domain-Specific Diffusion Models for Diversified Image Generations</strong></h1>
  <p id="authors"><a href="https://hmdliu.site/" target="_blank">Haoming Liu</a> <a href="https://ricercarg.github.io/" target="_blank">Yuanhe Guo</a> <a href="https://sheng-jie-wang.github.io/" target="_blank">Shengjie Wang</a> <a href="https://whongyi.github.io/" target="_blank">Hongyi Wen</a>
    <br><br>
    <span style="font-size: 20px"><a href="https://dail.shanghai.nyu.edu/" target="_blank">Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning, NYU Shanghai</a></span>
  </p>
  <br>
  <img src="./assets/header.png" class="teaser-gif" style="width:100%;">
    <font size="+2">
      <p style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.08873" target="_blank">Paper</a> &nbsp;&nbsp;&nbsp;&nbsp;
	      <a href="https://github.com/MAPS-research/Ditail" target="_blank">Code</a> &nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://huggingface.co/spaces/MAPS-research/Diffusion-Cocktail" target="_blank">Demo</a> &nbsp;&nbsp;&nbsp;&nbsp;
        <a href="./assets/bibtex.txt" target="_blank">BibTeX</a>
        </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Diffusion models, capable of high-quality image generation, receive unparalleled popularity for their ease of extension. Active users have created a massive collection of domain-specific diffusion models by fine-tuning base models on self-collected datasets. Recent work has focused on improving a single diffusion model by uncovering semantic and visual information encoded in various architecture components. However, those methods overlook the vastly available set of fine-tuned diffusion models and, therefore, miss the opportunity to utilize their combined capacity for novel generation. In this work, we propose <b>Di</b>ffusion Cock<b>tail</b> (<b>Ditail</b>), a training-free method that transfers style and content information between multiple diffusion models. This allows us to perform diversified generations using a set of diffusion models, resulting in novel images unobtainable by a single model. Ditail also offers fine-grained control of the generation process, which enables flexible manipulations of styles and contents. With these properties, Ditail excels in numerous applications, including style transfer guided by diffusion models, novel-style image generation, and image manipulation via prompts or collage inputs.</p>
  <p><b>TL;DR:</b> Ditail offers a training-free method for fine-grained manipulations of image content/style via the vastly available domain-specific Diffusion models and LoRAs checkpoints, 
  enabling diversified novel image generations.</p>
</div>
<div class="content">
  <h2>Background</h2>
  <p>Diffusion models (DMs) have shown great success in generating high-quality images, conditioning on various input types (e.g., text, reference image, etc.). Even though many recent works have focused on improving a single DM on generation controllability and interpretability of model components, rarely have people studied combinations of diffusion models, ignoring the vast collection of DMs created by everyday users. To the best of our knowledge, we are the first to study image generation using multiple DMs and the transfer of information in between. We propose Diffusion Cocktail (Ditail), a simple yet effective method that is readily applicable to existing DMs without further training/fine-tuning, thus allowing us to utilize the abundant and fast-growing DM resources efficiently and effectively.</p>
</div>
<div class="content">
  <h2>Method Overview</h2>
  <p>Ditail enables us to transform the source image (that is not necessarily generated by a DM) to some target domains, using domain-specific DM checkpoints as proxies. In other words, we explore a novel <i>model-centric style transfer</i> paradigm, where the target style is set according to a generative model, as opposed to a target style image in the conventional style transfer setting. <b>(a)</b> Ditail adopts a two-stage approach to produce the target image. We first invert the source image into noisy latents via DDIM, conditioning on a scaled interpolation of the prompt text embeddings (i.e., positive and negative prompts). Then, we perform denoising with the target DM while injecting the feature and self-attention maps (reconstructed from the source latents) into certain U-Net layers.  <b>(b)</b> Ditail can be naturally extended to transform collages to some target domain. <b>(c)</b> Ditail allows tailoring novel target styles by merging existing style DMs via checkpoint interpolation.</p>
  <img class="summary-img" src="./assets/overview.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Ditail for Novel Image Generation</h2>
  <p>Given <b>m</b> images generated by <b>m</b> DMs independently with the same textual prompt, one can identify images with desirable contents and styles, and apply Ditail to mix the content and styles for more satisfying results. Ditail can, in principle, generate <b>O(m<sup>2</sup>)</b> novel images that are not obtainable using a single model from the existing <b>m</b> DMs, significantly increasing the generation flexibility and diversity. The figure below demonstrates the cross-transfer over 3 DM checkpoints.</p>
  <img class="summary-img" src="./assets/style_matrix.png" style="width:70%;"> <br>
</div>
<!-- <div class="content">
  <h2>Ditail for Prompt-based Content Manipulation</h2>
  <p>Ditail enables prompt-based content manipulation through the prompt input for inversion and sampling. For each image pair within a prompt group, we use a smaller guidance scale for the left image and a larger guidance scale for the other. The appearance and semantic class of the objects can be modified effectively, even with completely unrelated prompts (e.g., man to chicken), whereas changing the number of objects may still fail due to the emphasis on structure.</p>
  <img class="summary-img" src="./assets/prompt_edit.png" style="width:100%;"> <br>
</div> -->
<div class="content">
  <h2>Qualitative Comparison of Style Transfer with Other Methods</h2>
  <img class="summary-img" src="./assets/sota.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>More Qualitative Results (DM+LoRA) on Real-world Images</h2>
  <img class="summary-img" src="./assets/lora.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Controllability, Extensibility, and Efficency</h2>
  <p>Ditail offers diverse controls over the fused generation process, including content manipulation (positive and negative prompts), structure preservation (scaling factors and content injection), style (DMs and LoRAs), and granularity (sampling steps and CFG factor). Moreover, Ditail is applicable to thousands of DMs and LoRAs available online, thereby enabling the vast community to <b>generate any content of any style</b>. With an Nvidia A10G-small GPU (i.e., our <a href="https://huggingface.co/spaces/MAPS-research/Diffusion-Cocktail-Demo" target="_blank">demo</a> on HuggingFace), processing a single image takes ~20 seconds for 50 sampling steps using 16-bit floating point precision. Please refer to our paper and code for more technical details.</p>
</div>
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements:</strong>
    This work is supported in part by the Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning at NYU Shanghai, NYU Shanghai Boost Fund, and NYU High Performance Computing resources and services. We use the following Diffusion model checkpoints: <a href="https://civitai.com/models/32333?modelVersionId=38765" target="_blank">BluePastel</a>, <a href="hhttps://civitai.com/models/91534?modelVersionId=97557" target="_blank">Chaos3.0</a>, <a href="https://civitai.com/models/75949?modelVersionId=87747" target="_blank">DiaMix</a>, <a href="https://civitai.com/models/15250?modelVersionId=96373" target="_blank">Little Illustration</a>, <a href="https://civitai.com/models/4201?modelVersionId=130072" target="_blank">Realistic Vision</a>, <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5" target="_blank">Stable Diffusion 1.5</a>. The presented source images are drawn from <a href="https://arxiv.org/abs/1504.00325v2" target="_blank">COCO Captions</a>, <a href="https://arxiv.org/abs/2210.08402" target="_blank">LAION-5B</a>, and <a href="https://arxiv.org/abs/2308.02205" target="_blank">GEMRec-18K</a>. The project page is adapted from <a href="https://dreambooth.github.io/" target="_blank">DreamBooth</a>.
  </p>
</div>
</body>
</html>
